---
title: "Practicing automated reporting"
author: "Template: Ian Hussey; content: [Student name]"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}

knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

options(es.use_symbols = TRUE) # get nice symbols when printing (on Windows requires R >= 4.2.0)

```

# Dependencies

```{r}

library(dplyr)
library(readr)
library(report) # part of {easystats}
library(see) # part of {easystats}
library(parameters) # part of {easystats}
library(correlation) # part of {easystats}
library(effectsize) # part of {easystats}
library(performance) # part of {easystats}
library(janitor)
library(lme4)
library(knitr)
library(kableExtra)

```

```{r}
# Load required libraries
library(tidyverse)

# Set seed for reproducibility
set.seed(42)

# Define sample sizes
n_control <- 27
n_sus <- 32
n_cbt <- 28
n_total <- n_control + n_sus + n_cbt

# Define session time points (every 3rd session, up to 21, plus 6-week follow-up)
time_points <- c(1, 4, 7, 10, 13, 16, 19, 21, "Follow-up")

# Create participant-level data with biased assignment
depression_data <- tibble(
  participant_id = 1:n_total,
  group = c(
    rep("Control", n_control), 
    rep("SUS", n_sus), 
    rep("CBT", n_cbt)
  ),
  baseline_BDI = c(
    rnorm(n_control, mean = 22, sd = 5), # Medium severity in control
    rnorm(n_sus, mean = 35, sd = 7),     # Severe cases sorted into SUS
    rnorm(n_cbt, mean = 15, sd = 4)      # Mild cases sorted into CBT
  )
) 

ggplot(long_depression_data, aes(x = runif(nrow(long_depression_data)), y = BDI_score)) +
  geom_point(alpha = 0.1, size = 5) +
  labs(title = "Completely Uninterpretable Depression Scores",
       x = "???",
       y = "BDI Score (But Who Even Knows)") +
  theme_minimal()


# Load required libraries
library(tidyverse)

# Set seed for reproducibility (so we get the same random data every time)
set.seed(123)

# Define number of participants per group
n_control <- 27  # No treatment
n_SUS <- 32      # Experimental treatment
n_CBT <- 28      # Standard therapy

# Define time points (every 3rd session + follow-up)
time_points <- c(1, 4, 7, 10, 13, 16, 19, 21, 27)  # 27 = 6-week follow-up

# Generate participant IDs
control_ids <- paste0("C", 1:n_control)
SUS_ids <- paste0("S", 1:n_SUS)
CBT_ids <- paste0("T", 1:n_CBT)

# Function to generate individual BDI score trajectories
generate_bdi_scores <- function(start_bdi, time_points, decline_rate) {
  # Create a decreasing trajectory with some random noise
  return(start_bdi - (decline_rate * time_points) + rnorm(length(time_points), 0, 2))
}

long_depression_data <- depression_data %>%
  expand(participant_id, time = time_points) %>%
  left_join(depression_data, by = "participant_id") %>%
  mutate(
    time = factor(time, levels = time_points),  # Ensure time is ordered correctly
    session_number = as.numeric(as.character(time)),  # Convert to numeric where possible
    session_number = replace_na(session_number, max(session_number, na.rm = TRUE) + 3),  # Set follow-up session correctly
    change_factor = case_when(
      group == "Control" ~ -5 * (session_number / max(session_number)),  # Slight improvement
      group == "SUS" ~ -15 * (session_number / max(session_number)),     # Biggest improvement
      group == "CBT" ~ -10 * (session_number / max(session_number))      # Moderate improvement
    ),
    BDI_score = baseline_BDI + change_factor + rnorm(n(), mean = 0, sd = 2) # Add random variation
  ) 



# Create datasets for each group
control_data <- map2_dfr(control_ids, initial_control, ~ tibble(
  participant_id = .x,
  group = "Control",
  time = time_points,
  BDI_score = generate_bdi_scores(.y, time_points, decline_control[match(.x, control_ids)])
))

SUS_data <- map2_dfr(SUS_ids, initial_SUS, ~ tibble(
  participant_id = .x,
  group = "SUS",
  time = time_points,
  BDI_score = generate_bdi_scores(.y, time_points, decline_SUS[match(.x, SUS_ids)])
))

CBT_data <- map2_dfr(CBT_ids, initial_CBT, ~ tibble(
  participant_id = .x,
  group = "CBT",
  time = time_points,
  BDI_score = generate_bdi_scores(.y, time_points, decline_CBT[match(.x, CBT_ids)])
))

# Combine all groups into a single dataset
long_depression_data <- bind_rows(control_data, SUS_data, CBT_data)

# ---------------------- METHOD 1: Normalization by Subtracting Baseline ----------------------

# Subtract the initial BDI score from all measurements for each participant
long_depression_data <- long_depression_data %>%
  group_by(participant_id) %>%
  mutate(relative_BDI = BDI_score - first(BDI_score)) %>% 
  ungroup()

# ---------------------- METHOD 2: Normalization by Percentage Improvement ----------------------

# Convert each BDI score into % improvement relative to baseline
long_depression_data <- long_depression_data %>%
  group_by(participant_id) %>%
  mutate(relative_BDI = BDI_score - first(BDI_score)) %>%
  ungroup()

# ---------------------- PLOTTING (Normalized Data) ----------------------

# Method 1: Relative BDI Change (Baseline = 0)
ggplot(long_depression_data, aes(x = factor(time), y = relative_BDI, fill = group)) +
  stat_summary(fun = mean, geom = "bar", position = "dodge", width = 0.7) +  # Bar plot with dodge for separation
  labs(title = "Depression Severity Change Over Time (Baseline Normalized)",
       x = "Session (every 3rd session, plus follow-up)",
       y = "Change in BDI Score from Baseline") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability


# Method 2: Percentage Improvement (Higher = More Recovery)
ggplot(long_depression_data, aes(x = time, y = percent_improvement, color = group)) +
  stat_summary(fun = mean, geom = "line", aes(group = group), linewidth = 1.5) +
  labs(title = "Percentage Improvement in Depression Symptoms",
       x = "Session (every 3rd session, plus follow-up)",
       y = "Percent Improvement from Baseline (%)") +
  theme_minimal()


```
# Inference tests

## Regressions

```{r}

# fit model
model <- lm(wt ~ 1 + am + mpg, data = mtcars)

# report - text output (nb omits intercept!)
report(model)

# each parameter (including intercept)
report_parameters(model)

# just parameters in text format
report_statistics(model)

# just parameters in table format
parameters(model)

# just parameters in table html format 
parameters(model) |>
  mutate(p = insight::format_p(p)) |>
  mutate_if(is.numeric, round_half_up, digits = 2) |>
  kable() |>
  kable_classic(full_width = FALSE)

# table in markdown format
report_table(model)

# table in html format - needs to be rounded manually
report_table(model) |>
  mutate(p = insight::format_p(p)) |>
  mutate_if(is.numeric, round_half_up, digits = 2) |>
  kable() |>
  kable_classic(full_width = FALSE)

# plot
parameters(model) |>
  plot() 

```

## Correlations

### Single correlation tests

```{r}

# fit model
model <- cor.test(mtcars$mpg, mtcars$wt)

# report - text output 
report(model)

# table in html format - needs to be rounded manually
report_table(model) |>
  mutate(p = insight::format_p(p)) |>
  mutate_if(is.numeric, round_half_up, digits = 2) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

### Many 

```{r}

results <- correlation(iris)

results

results %>%
  summary(redundant = TRUE) %>%
  plot()

```

### By group

```{r}

iris %>%
  select(Species, Sepal.Length, Sepal.Width, Petal.Width) %>%
  group_by(Species) %>%
  correlation()

```

## t-tests

NB Cohen's d is approximated - better to calculate it separately and accurately.

```{r}

# fit model
model <- t.test(mpg ~ am, data = mtcars)

# report - text output 
report(model)

# table in html format - needs to be rounded manually
report_table(model) |>
  mutate(p = insight::format_p(p)) |>
  mutate_if(is.numeric, round_half_up, digits = 2) |>
  kable() |>
  kable_classic(full_width = FALSE)


# estimate Cohen's d directly from data
cohens_d(mpg ~ am, data = mtcars)

```

## Multilevel/hierarchical/mixed models

```{r}

# fit model
model <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)

# parameters in text format 
report(model)

# parameters in table format
parameters(model) |>
  mutate(p = insight::format_p(p)) |>
  mutate_if(is.numeric, round_half_up, digits = 2) |>
  kable() |>
  kable_classic(full_width = FALSE)

# table in html format - needs to be rounded manually
report_table(model) |>
  mutate(p = insight::format_p(p)) |>
  mutate_if(is.numeric, round_half_up, digits = 2) |>
  kable() |>
  kable_classic(full_width = FALSE)

# plot
parameters(model) |>
  plot() 

# check assumptions of random effects
result <- check_normality(model, effects = "random")
plot(result)

```

## ANOVAs

```{r}

# fit model
model <- aov(mpg ~ factor(gear) + factor(carb), data = mtcars)

# commonly used effect size: partial eta squared
eta_squared(model)

# better effect size: partialomega squared
omega_squared(model)


```

# Summary statistics

```{r}

# all columns
iris |>
  group_by(Species) |>
  report_table() 

# all columns - html output with rounding
iris |>
  group_by(Species) |>
  report_table() |>
  mutate_if(is.numeric, round_half_up, digits = 2) |>
  kable() |>
  kable_classic(full_width = FALSE)

# subset of columns
iris |>
  group_by(Species) |>
  report_table() |>
  select(Group, Variable, n_Obs, Mean, SD) |>
  mutate_if(is.numeric, round_half_up, digits = 2) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# Assumption checks

Beware that checking assumptions can lead to as many bad practices as it does good ones! (e.g., poorly justified post hoc outlier exclusion)

## Multiple checks at once

```{r fig.height=8, fig.width=8}

# fit model
model <- lm(wt ~ 1 + am + mpg, data = mtcars)

# check multiple model assumptions
check_model(model)

```

## Normality of distribution of residuals

```{r}

res_normality <- check_normality(model)

res_normality
plot(res_normality, type = "qq")
plot(res_normality, type = "density")

```

## Multicolinearity

```{r}

res_collinearity <- check_collinearity(model)

res_collinearity
plot(res_collinearity)

```

## Outliers

```{r}

res_outliers <- check_outliers(model, method = "cook") # "all" requires other dependencies and can take some time to run  
#res_outliers <- check_outliers(model, method = "all") # "all" requires other dependencies and can take some time to run  

res_outliers
plot(res_outliers)

```

## Heteroscedasticity

```{r}

res_het <- check_heteroscedasticity(model)

res_het
plot(res_het)

```

# Session info

```{r}

sessionInfo()

```

